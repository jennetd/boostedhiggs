{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "import json\n",
    "import uproot3\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "from coffea import processor, util, hist\n",
    "import pickle\n",
    "\n",
    "from plotter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumis = {}\n",
    "lumis['2016'] = 35.9\n",
    "lumis['2017'] = 41.5\n",
    "lumis['2018'] = 59.9\n",
    "\n",
    "with open('xsec.json') as f:\n",
    "  xs = json.load(f)\n",
    "\n",
    "with open('pmap.json') as f:\n",
    "  pmap = json.load(f)\n",
    "\n",
    "systematics = ['nominal',\n",
    "               'jet_triggerUp','jet_triggerDown',\n",
    "               'btagWeightUp','btagWeightDown','btagEffStatUp','btagEffStatDown',\n",
    "               'UESUp','UESDown','JESUp','JESDown','JERUp','JERDown',\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2017'\n",
    "nfiles = len(subprocess.getoutput(\"ls infiles-split/\"+year+\"*.json\").split())\n",
    "outsum = processor.dict_accumulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repickle=True\n",
    "\n",
    "# Check if pickle exists, and don't re-create it if it does\n",
    "picklename = 'pickles/templates.pkl'\n",
    "if os.path.isfile(picklename):\n",
    "    repickle=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files - this takes a while\n",
    "if repickle:\n",
    "    nfiles = len(subprocess.getoutput(\"ls infiles-split/\"+year+\"*.json\").split())\n",
    "    for n in range(1,nfiles+1):\n",
    "\n",
    "        with open('infiles-split/'+year+'_'+str(n)+'.json') as f:\n",
    "            infiles = json.load(f)\n",
    "    \n",
    "        filename = '/myeosdir/vh-category/outfiles/'+year+'_'+str(n)+'.coffea'\n",
    "        #filename = 'outfiles/'+year+'_'+str(n)+'.coffea'\n",
    "        if os.path.isfile(filename):\n",
    "            out = util.load(filename)\n",
    "            outsum.add(out)\n",
    "        else:\n",
    "            print('Missing file '+str(n),infiles.keys())\n",
    "            #print(\"File \" + filename + \" is missing\")\n",
    "        \n",
    "    scale_lumi = {k: xs[k] * 1000 *lumis[year] / w for k, w in outsum['sumw'].items()}\n",
    "    outsum['templates'].scale(scale_lumi, 'dataset')\n",
    "    \n",
    "    # Use pmap to group the datasets together\n",
    "    templates = outsum['templates'].group('dataset', hist.Cat('process', 'Process'), pmap)\n",
    "\n",
    "    outfile = open(picklename, 'wb')\n",
    "    pickle.dump(templates, outfile, protocol=-1)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the histogram from the pickle file\n",
    "templates = pickle.load(open(picklename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check intergrals\n",
    "templates.sum('pt1','pt2','msd1','msd2','ddb1','ddb2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_vh = templates.sum('pt2','ddb2').integrate('msd2',int_range=slice(68,103)).integrate('region','signal')\n",
    "templates_vh_mucr = templates.sum('pt1','pt2','msd2','ddb2').integrate('region','muoncontrol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('rm '+year+'/1-signalregion.root')\n",
    "fout = uproot3.create(year+'/1-signalregion.root')\n",
    "for p in pmap.keys():  \n",
    "    if p == \"ttH\" and year == '2016':\n",
    "        continue\n",
    "    print(p)\n",
    "    if \"data\" in p:\n",
    "        s = \"nominal\"\n",
    "        h = templates_vh.sum('pt1').integrate('systematic',s).integrate('ddb1',int_range=slice(0.89,1)).integrate('process',p)\n",
    "        fout[\"pass_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "        h = templates_vh.sum('pt1').integrate('systematic',s).integrate('ddb1',int_range=slice(0,0.89)).integrate('process',p)\n",
    "        fout[\"fail_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "    else:\n",
    "        for s in systematics:\n",
    "            h = templates_vh.sum('pt1').integrate('systematic',s).integrate('ddb1',int_range=slice(0.89,1)).integrate('process',p)\n",
    "            fout[\"pass_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "            h = templates_vh.sum('pt1').integrate('systematic',s).integrate('ddb1',int_range=slice(0,0.89)).integrate('process',p)\n",
    "            fout[\"fail_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbins = [450, 550, 1200]\n",
    "os.system('rm '+year+'/2pt-signalregion.root')\n",
    "fout = uproot3.create(year+'/2pt-signalregion.root')\n",
    "\n",
    "for i,b in enumerate(ptbins[:-1]):\n",
    "    for p in pmap.keys(): \n",
    "        if p == \"ttH\" and year == '2016':\n",
    "            continue\n",
    "        print(p)\n",
    "        if \"data\" in p:\n",
    "            s = \"nominal\"\n",
    "            h = templates_vh.integrate('systematic',s).integrate('pt1',int_range=slice(ptbins[i],ptbins[i+1])).integrate('ddb1',int_range=slice(0.89,1)).integrate('process',p)\n",
    "            fout[\"pass_pt\"+str(i+1)+\"_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "            h = templates_vh.integrate('systematic',s).integrate('pt1',int_range=slice(ptbins[i],ptbins[i+1])).integrate('ddb1',int_range=slice(0,0.89)).integrate('process',p)\n",
    "            fout[\"fail_pt\"+str(i+1)+\"_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "        else:\n",
    "            for s in systematics:\n",
    "                h = templates_vh.integrate('systematic',s).integrate('pt1',int_range=slice(ptbins[i],ptbins[i+1])).integrate('ddb1',int_range=slice(0.89,1)).integrate('process',p)\n",
    "                fout[\"pass_pt\"+str(i+1)+\"_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "                h = templates_vh.integrate('systematic',s).integrate('pt1',int_range=slice(ptbins[i],ptbins[i+1])).integrate('ddb1',int_range=slice(0,0.89)).integrate('process',p)\n",
    "                fout[\"fail_pt\"+str(i+1)+\"_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('rm '+year+'/muonCR.root')\n",
    "fout = uproot3.create(year+'/muonCR.root')\n",
    "for p in pmap.keys():  \n",
    "    if p == 'ttH' and year == '2016':\n",
    "        continue\n",
    "    print(p)\n",
    "    if \"data\" in p:\n",
    "        s = \"nominal\"\n",
    "        h = templates_vh_mucr.integrate('systematic',s).integrate('ddb1',int_range=slice(0.89,1)).integrate('process',p)\n",
    "        fout[\"pass_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "        h = templates_vh_mucr.integrate('systematic',s).integrate('ddb1',int_range=slice(0,0.89)).integrate('process',p)\n",
    "        fout[\"fail_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "    else:\n",
    "        for s in systematics:\n",
    "            h = templates_vh_mucr.integrate('systematic',s).integrate('ddb1',int_range=slice(0.89,1)).integrate('process',p)\n",
    "            fout[\"pass_\"+p+\"_\"+s] = hist.export1d(h)\n",
    "            h = templates_vh_mucr.integrate('systematic',s).integrate('ddb1',int_range=slice(0,0.89)).integrate('process',p)\n",
    "            fout[\"fail_\"+p+\"_\"+s] = hist.export1d(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
